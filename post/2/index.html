<!DOCTYPE html>
<html>
  <head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-173578745-1"></script>
    <script>
      if (window.location.host==="charx7.github.io" || window.location.host === "www.charx7.github.io") {
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'UA-173578745-1');
      }
    </script>

    
      <title>Carlos Portfolio - Static website using python, flask, and frozen-flask (1)</title>
    
    <!-- Bootstrap -->
    <link rel="stylesheet" type="text/css" 
      href="/static//css/bootstrap.css">
    <link rel="stylesheet" type="text/css"
      href="/static//css/main.css">  
      <!-- Font Awsome CDN -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
    
    <link rel="stylesheet" type="text/css"
      href="/static//css/colorful.css">
    
  </head>

  <!-- Pigments for code CSS -->
  

  <body>
    <header class="site-header"></header>

    <nav class="navbar navbar-expand-md navbar-dark bg-steel fixed-top">
      <div class="container">
        <a class="navbar-brand mr-4" href="/">Carlos Huerta</a>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarToggle" aria-controls="navbarToggle" aria-expanded="false" aria-label="Toggle navigation">
          <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse" id="navbarToggle">
          <div class="navbar-nav mr-auto">
            <a class="nav-item nav-link" href="/about.html">About</a>
            <a class="nav-item nav-link" href="/index.html">Blog</a>
            <a class="nav-item nav-link" href="/portfolio.html">Portfolio</a>
          </div>
          <!-- Navbar Right Side 
          <div class="navbar-nav">
            <a class="nav-item nav-link" href="/login">Login</a>
            <a class="nav-item nav-link" href="/register">Register</a>
          </div>
          -->
        </div>
      </div>
    </nav>

    <!-- Paralax Image-->
    <div class = "parallax">
      <!-- Paralax Text-->
      <div class = "container-title-text">
        <h1 id="parallax-title">Carlos Huerta - Data Science Blog</h1>
      </div>
    </div>
    
    <!-- Main content-->
    <div class="main-content">
      <div class="container">
        
  <div class="content-section">
    <h2>Parallel fp-growth algorithm using spark, docker and mongodb (1)</h2>
    <p>By Carlos Huerta on Jul 27 2020</p>
    <div><h3>How to build an scalable recomendation engine ⚙️</h3>
<p>Last Year in collaboration with <a  target="_blank" href="https://www.linkedin.com/in/ankit001mittal/">Ankit Mittal</a> and <a target="_blank" href="https://www.linkedin.com/in/myuja/">Michael Yuja</a>  we had a project in mind. To develop a <strong>recommendation engine</strong> for the e-commerce website www.agglobal.com that sells hardware solutions in Honduras. The website has listed more than 7000 products and can be very hard for the daily user to navigate around all the different products and categories for them to find what they need.</p>
<p>Fortunately, we can resort to big-data and data science solutions to solve this issue. 
Similar to Amazon's "People who also bought X also bought Y..." that displays when browsing through Amazon's humongous catalog; to achieve that, we seek to develop a similar solution.</p>
<h2>1. Set-up the spark-cluster using docker</h2>
<p>We are going to use the <strong>Big Data Europe</strong> docker-spark container images as base images for our project. Nevertheless we need to set-up a folder structure as follows:</p>
<h3>1.1 Spark-master set-up</h3>
<p>Create a <code>spark_master</code> directory, inside it create a <code>dockerfile</code> and a <code>master.sh</code> file. </p>
<div class="input">
  MY_PROJECT_NAME/spark_master/dockerfile
</div>

<div class="codehilite"><pre><span></span><code><span class="k">FROM</span> <span class="s">bde2020/spark-base:2.4.0-hadoop2.7</span>

<span class="k">COPY</span> master.sh /

<span class="k">ENV</span> SPARK_MASTER_PORT <span class="m">7077</span>
<span class="k">ENV</span> SPARK_MASTER_WEBUI_PORT <span class="m">8080</span>
<span class="k">ENV</span> SPARK_MASTER_LOG /spark/logs

<span class="k">EXPOSE</span><span class="s"> 8080 7077 6066</span>

<span class="k">CMD</span> <span class="p">[</span><span class="s2">&quot;/bin/bash&quot;</span><span class="p">,</span> <span class="s2">&quot;/master.sh&quot;</span><span class="p">]</span>
</code></pre></div>


<p>The <code>dockerfile</code> pulls a base docker image from dockerhub and sets some necessary environmental variables. </p>
<ul>
<li><code>SPARK_MASTER_PORT</code>: defines in which port (inside the docker container) will our spark master run.</li>
<li><code>SPARK_MASTER_WEBUI_PORT</code>: defines the port to access the web UI.</li>
<li><code>SPARK_MASTER_LOG</code>: the logs route.</li>
</ul>
<p><code>EXPOSE</code> tells which container ports we are going to expose. <code>8080</code> for the web UI, <code>7077</code> to connect to the spark-master and <code>6066</code> is the spark-master port for the REST URL.</p>
<div class="input">
  MY_PROJECT_NAME/spark_master/master.sh
</div>

<div class="codehilite"><pre><span></span><code><span class="ch">#!/bin/bash</span>

<span class="nb">export</span> <span class="nv">SPARK_MASTER_HOST</span><span class="o">=</span><span class="sb">`</span>hostname<span class="sb">`</span>

<span class="nb">source</span> <span class="s2">&quot;/spark/sbin/spark-config.sh&quot;</span>

<span class="nb">source</span> <span class="s2">&quot;/spark/bin/load-spark-env.sh&quot;</span>

mkdir -p <span class="nv">$SPARK_MASTER_LOG</span>

<span class="nb">export</span> <span class="nv">SPARK_HOME</span><span class="o">=</span>/spark

ln -sf /dev/stdout <span class="nv">$SPARK_MASTER_LOG</span>/spark-master.out

<span class="nb">cd</span> /spark/bin <span class="o">&amp;&amp;</span> /spark/sbin/../bin/spark-class org.apache.spark.deploy.master.Master <span class="se">\</span>
--ip <span class="nv">$SPARK_MASTER_HOST</span> --port <span class="nv">$SPARK_MASTER_PORT</span> --webui-port <span class="nv">$SPARK_MASTER_WEBUI_PORT</span> &gt;&gt; <span class="nv">$SPARK_MASTER_LOG</span>/spark-master.out
</code></pre></div>


<p>This bash script executes spark scripts that will load the configuration and the spark environment; you can inspect them by executing a shell inside the container(more on this later as it is a handy debugging tool for docker related issues), then makes a directory for our logs and sets the necessary <code>SPARK_HOME</code> environmental variable inside our docker container. Finally, it will execute spark binaries using the previously defined environmental variables. TLDR: This script executes our spark-master for us.</p>
<h3>1.2 Spark-workers set-up</h3>
<p>In the root of your project create a directory <code>spark_worker</code> and create a <code>dockerfile</code>.</p>
<div class="input">
  MY_PROJECT_NAME/spark_worker/dockerfile
</div>

<div class="codehilite"><pre><span></span><code><span class="k">FROM</span> <span class="s">bde2020/spark-base:2.4.0-hadoop2.7</span>

<span class="k">COPY</span> worker.sh /

<span class="k">ENV</span> SPARK_WORKER_WEBUI_PORT <span class="m">8081</span>
<span class="k">ENV</span> SPARK_WORKER_LOG /spark/logs
<span class="k">ENV</span> SPARK_MASTER <span class="s2">&quot;spark://spark-master:7077&quot;</span>

<span class="k">EXPOSE</span><span class="s"> 8081</span>

<span class="c"># Copy the requirements.txt first, for separate dependency resolving and downloading</span>
<span class="c"># the -p is to make it recursive</span>
<span class="k">RUN</span> mkdir -p /app
<span class="k">COPY</span> requirements.txt /app/
<span class="k">RUN</span> <span class="nb">cd</span> /app <span class="se">\</span>
    <span class="o">&amp;&amp;</span> pip3 install -r requirements.txt

<span class="c"># Configure the following environment variables (unless the default value satisfies):</span>
<span class="k">ENV</span> SPARK_MASTER_NAME <span class="s2">&quot;spark-master&quot;</span>
<span class="k">ENV</span> SPARK_MASTER_PORT <span class="s2">&quot;7077&quot;</span>

<span class="k">CMD</span> <span class="p">[</span><span class="s2">&quot;/bin/bash&quot;</span><span class="p">,</span> <span class="s2">&quot;/worker.sh&quot;</span><span class="p">]</span>
</code></pre></div>


<p>For this dockerfile, we need to set the <code>SPARK_MASTER`` SPARK_WORKER_WEBUI_PORT</code> and <code>SPARK_WORKER_LOG</code> environmental variables, we also copy and install requirements declared on a requirements.txt file (OPTIONAL), this could be useful if your project has external dependencies that you wish to install on every worker. We will be exposing the port <code>8081</code> inside our container and will define the map to the external port on the docker=compose file (part 2).</p>
<p>As in the <code>spark_master</code> directory, we will also add a script file.</p>
<div class="input">
  MY_PROJECT_NAME/spark_worker/worker.sh
</div>

<div class="codehilite"><pre><span></span><code><span class="ch">#!/bin/bash</span>

. <span class="s2">&quot;/spark/sbin/spark-config.sh&quot;</span>

. <span class="s2">&quot;/spark/bin/load-spark-env.sh&quot;</span>

mkdir -p <span class="nv">$SPARK_WORKER_LOG</span>

<span class="nb">export</span> <span class="nv">SPARK_HOME</span><span class="o">=</span>/spark

ln -sf /dev/stdout <span class="nv">$SPARK_WORKER_LOG</span>/spark-worker.out

<span class="c1"># Toggle to debug a greater sleep, some sleep is needed to give time to the master </span>
sleep <span class="m">20</span>

<span class="c1"># Execute the connection to the master</span>
/spark/sbin/../bin/spark-class org.apache.spark.deploy.worker.Worker <span class="se">\</span>
 --webui-port <span class="nv">$SPARK_WORKER_WEBUI_PORT</span> <span class="nv">$SPARK_MASTER</span> <span class="se">\</span>
 --memory 1G --cores <span class="m">1</span>  &gt;&gt; <span class="nv">$SPARK_WORKER_LOG</span>/spark-worker.out
</code></pre></div>


<p>This script will also source the configuration and env scripts, declare our SPARK_HOME environmental variable and finally call the spark binary that registers our spark-worker to the master. You can also modify the amount of <strong>memory</strong> and <strong>cores</strong> assigned to each worker.</p>
<h3>1.3 Spark-submit and spark-job set-up</h3>
<p>To submit a spark job with the appropriate structure and dependencies, we need to create a <em>python package egg</em>, so it will be necessary to create a python-package structure inside our project, but first we should create a venv that will be used to build and create our package and install (locally) our dependencies.</p>
<div class="input">
  MY_PROJECT_NAME/pyspark_src/
</div>

<div class="codehilite"><pre><span></span><code>tree
.
├── Dockerfile
├── pyspark_recom_engine
│   ├── __init__.py
│   └── <span class="nb">jobs</span>
├── requirements.txt
├── setup.py
└── template.sh
</code></pre></div>


<p>Here the directory <code>pyspark_recom_engine</code> will be where the logic of our package will live. In order for this to work, we need to set-up the following files:</p>
<div class="input">
  MY_PROJECT_NAME/pyspark_src/Dockerfile
</div>

<div class="codehilite"><pre><span></span><code><span class="c"># Will extend form the spark-template that extends on itself with the spark-submit image</span>
<span class="c"># FROM bde2020/spark-python-template:2.4.0-hadoop2.7</span>
<span class="c"># Using an image before (the one that has the python reqs install)</span>
<span class="k">FROM</span> <span class="s">bde2020/spark-submit:2.4.0-hadoop2.7</span> 

<span class="k">COPY</span> template.sh /

<span class="c"># Copy the requirements.txt first, for separate dependency resolving and downloading</span>
<span class="k">COPY</span> requirements.txt /app/
<span class="c">#RUN pip3 install --upgrade pip</span>
<span class="k">RUN</span> <span class="nb">cd</span> /app <span class="se">\</span>
      <span class="o">&amp;&amp;</span> pip3 install -r requirements.txt

<span class="c"># Copy the source code</span>
<span class="k">COPY</span> . /app

<span class="c"># Needed params</span>
<span class="k">ENV</span> SPARK_MASTER_NAME <span class="s2">&quot;spark-master&quot;</span>
<span class="k">ENV</span> SPARK_MASTER_PORT <span class="s2">&quot;7077&quot;</span>
<span class="c"># Not sure if will work with this (this should be built by a parent image!)</span>
<span class="k">ENV</span> SPARK_MASTER_URL <span class="s2">&quot;spark://spark-master:7077&quot;</span>
<span class="c"># The location of the Job</span>
<span class="k">ENV</span> SPARK_APPLICATION_PYTHON_LOCATION app/pyspark_recom_engine/jobs/FpJob.py
<span class="c"># Extra (In case we need to suplement our spark job with command line args)</span>
<span class="k">ENV</span> SPARK_APPLICATION_ARGS <span class="s2">&quot;foo bar baz&quot;</span>
<span class="c"># Add the python egg for inter-package dependancies </span>
<span class="c"># on the --packages flag we add spark dependancies that otherwise would be built by sbt</span>
<span class="k">ENV</span> <span class="nv">SPARK_SUBMIT_ARGS</span><span class="o">=</span><span class="s2">&quot;--packages org.mongodb.spark:mongo-spark-connector_2.11:2.4.0 --py-files app/dist/pyspark_recom_engine-0.1-py3.6.egg&quot;</span>

<span class="k">CMD</span> <span class="p">[</span><span class="s2">&quot;/bin/bash&quot;</span><span class="p">,</span><span class="s2">&quot;/template.sh&quot;</span><span class="p">,</span><span class="s2">&quot;/submit.sh&quot;</span><span class="p">]</span>
</code></pre></div>


<p>Unlike our previous Dockerfiles this one is a little bit more complicated, but bear with me. At first we will be extending our image from the base BDE spark images and copying the entire source code into the container. Then we need to define the following environmental variables inside the container:</p>
<ul>
<li><code>SPARK_MASTER_NAME</code>: Defines the name given to our spak-master, we could have changed this variable inside our docker image for our spark-master.</li>
<li><code>SPARK_MASTER_PORT</code>: The port inside the container from which the spark-master is being executed.</li>
<li><code>SPARK_MASTER_URL</code>: The URL to access the spark master.</li>
<li><code>SPARK_APPLICATION_PYTHON_LOCATION</code>: Sets the location of the job that we will be submitting into the spark-master</li>
<li><code>SPARK_APPLICATION_ARGS</code>: Defines a set of arguments in case we are using an argument parser inside our spark-job</li>
<li><code>SPARK_SUBMIT_ARGS</code>: The actual submit arguments that will be run by the submit bash script. Since we are going to be using mongodb in the future we are going to import the spark mongo connector. More on how <code>pyspark_recom_engine-0.1-py3.6.egg</code> gets built will follow.</li>
</ul>
<p>That was a lot, wasn't it? Well, we aren't finished yet, we still need to define a few more files.</p>
<div class="input">
  MY_PROJECT_NAME/pyspark_src/setup.py
</div>

<div class="codehilite"><pre><span></span><code><span class="kn">from</span> <span class="nn">setuptools</span> <span class="kn">import</span> <span class="n">setup</span> <span class="p">,</span> <span class="n">find_packages</span>

<span class="c1"># Parse the requirements form a requirements.txt file</span>
<span class="k">def</span> <span class="nf">parse_requirements</span><span class="p">(</span><span class="n">filename</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; load requirements from a pip requirements file &quot;&quot;&quot;</span>
    <span class="n">lineiter</span> <span class="o">=</span> <span class="p">(</span><span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="nb">open</span><span class="p">(</span><span class="n">filename</span><span class="p">))</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">line</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">lineiter</span> <span class="k">if</span> <span class="n">line</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">line</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;#&quot;</span><span class="p">)]</span>

<span class="n">requirements</span> <span class="o">=</span> <span class="n">parse_requirements</span><span class="p">(</span><span class="s1">&#39;requirements.txt&#39;</span><span class="p">)</span>

<span class="n">setup</span><span class="p">(</span>
    <span class="n">name</span><span class="o">=</span><span class="s2">&quot;pyspark_recom_engine&quot;</span><span class="p">,</span>
    <span class="n">version</span><span class="o">=</span><span class="s2">&quot;0.1&quot;</span><span class="p">,</span>
    <span class="n">author</span><span class="o">=</span><span class="s2">&quot;&lt;YOUR_NAME_HERE&gt;&quot;</span><span class="p">,</span>
    <span class="n">author_email</span><span class="o">=</span><span class="s2">&quot;&lt;YOUR_EMAIL_HERE&gt;&quot;</span><span class="p">,</span>
    <span class="n">description</span><span class="o">=</span><span class="s2">&quot;Spark fp-growth recommendation engine using spark&quot;</span><span class="p">,</span>
    <span class="n">url</span><span class="o">=</span><span class="s2">&quot;&lt;YOUR_URL_HERE&gt;&quot;</span><span class="p">,</span>
    <span class="n">packages</span><span class="o">=</span><span class="n">find_packages</span><span class="p">(),</span>
    <span class="n">install_requires</span><span class="o">=</span><span class="n">requirements</span><span class="p">,</span>
    <span class="n">classifiers</span><span class="o">=</span><span class="p">[</span>
        <span class="s2">&quot;Programming Language :: Python :: 3&quot;</span><span class="p">,</span>
        <span class="s2">&quot;License :: OSI Approved :: MIT License&quot;</span><span class="p">,</span>
        <span class="s2">&quot;Operating System :: OS Independent&quot;</span><span class="p">,</span>
    <span class="p">],</span>
<span class="p">)</span>
</code></pre></div>


<p>We are building a python package so we need a <code>setup.py</code> file that we will use to build our project into a python egg. The <code>name</code> variable could be anything, but since we created a <code>pyspark_recom_engine</code> directory, we will name our package that way. Remember the <code>pyspark_recom_engine-0.1-py3.6.egg</code> well, this is how it gets built, the <code>setup.py</code> file generates that name using the given <code>version</code> and <code>name</code> arguments as well as the current version of python in which it was built, this is why it is always a good idea to isolate your packages inside a virtual environment. </p>
<p>Finally the <code>template.sh</code> file:</p>
<div class="input">
  MY_PROJECT_NAME/pyspark_src/template.sh
</div>

<div class="codehilite"><pre><span></span><code><span class="ch">#!/bin/bash</span>

sh /submit.sh
</code></pre></div>


<p>Is just a simple script that will execute the <code>submit.sh</code> script that was inherited from a base submit image.</p>
<h3>1.4 Putting it all together</h3>
<p>We are really close but not there yet... on the root of our project we need to create the following <strong>docker-compose</strong> file.</p>
<div class="input">
  MY_PROJECT_NAME/docker-compose.yml
</div>

<div class="codehilite"><pre><span></span><code><span class="k">version</span><span class="p">:</span> <span class="ss">&quot;3&quot;</span>

<span class="n">services</span><span class="p">:</span>
  <span class="n">spark</span><span class="o">-</span><span class="n">mongo</span><span class="p">:</span>
    <span class="n">image</span><span class="p">:</span> <span class="n">mongo</span>
    <span class="n">ports</span><span class="p">:</span>
      <span class="o">-</span> <span class="ss">&quot;27017:27017&quot;</span>
    <span class="n">networks</span><span class="p">:</span>
      <span class="o">-</span> <span class="n">spark</span><span class="o">-</span><span class="n">network</span>
    <span class="n">command</span><span class="p">:</span> <span class="n">mongod</span>
    <span class="n">volumes</span><span class="p">:</span>
      <span class="o">-</span> <span class="n">mongodb</span><span class="p">:</span><span class="o">/</span><span class="k">data</span><span class="o">/</span><span class="n">db</span>
      <span class="o">-</span> <span class="n">mongodb_config</span><span class="p">:</span><span class="o">/</span><span class="k">data</span><span class="o">/</span><span class="n">configdb</span>
  <span class="n">spark</span><span class="o">-</span><span class="n">master</span><span class="p">:</span>
    <span class="n">build</span><span class="p">:</span>
      <span class="n">context</span><span class="p">:</span> <span class="p">.</span><span class="o">/</span><span class="n">spark_master</span><span class="o">/</span>
    <span class="n">ports</span><span class="p">:</span>
      <span class="o">-</span> <span class="ss">&quot;7077:7077&quot;</span>
      <span class="o">-</span> <span class="ss">&quot;8080:8080&quot;</span>
    <span class="n">networks</span><span class="p">:</span>
      <span class="o">-</span> <span class="n">spark</span><span class="o">-</span><span class="n">network</span>
    <span class="n">depends_on</span><span class="p">:</span>
      <span class="o">-</span> <span class="n">spark</span><span class="o">-</span><span class="n">mongo</span>
  <span class="n">spark</span><span class="o">-</span><span class="n">worker</span><span class="o">-</span><span class="mi">1</span><span class="p">:</span>
    <span class="n">build</span><span class="p">:</span>
      <span class="n">context</span><span class="p">:</span> <span class="p">.</span><span class="o">/</span><span class="n">pyspark_worker</span><span class="o">/</span>
    <span class="n">ports</span><span class="p">:</span>
      <span class="o">-</span> <span class="ss">&quot;8081:8081&quot;</span>
    <span class="n">networks</span><span class="p">:</span>
      <span class="o">-</span> <span class="n">spark</span><span class="o">-</span><span class="n">network</span>
    <span class="n">depends_on</span><span class="p">:</span>
      <span class="o">-</span> <span class="n">spark</span><span class="o">-</span><span class="n">master</span>
      <span class="o">-</span> <span class="n">spark</span><span class="o">-</span><span class="n">mongo</span>
  <span class="n">spark</span><span class="o">-</span><span class="n">worker</span><span class="o">-</span><span class="mi">2</span><span class="p">:</span>
    <span class="n">build</span><span class="p">:</span>
      <span class="n">context</span><span class="p">:</span> <span class="p">.</span><span class="o">/</span><span class="n">pyspark_worker</span><span class="o">/</span>
    <span class="n">ports</span><span class="p">:</span>
      <span class="o">-</span> <span class="ss">&quot;8082:8081&quot;</span>
    <span class="n">networks</span><span class="p">:</span>
      <span class="o">-</span> <span class="n">spark</span><span class="o">-</span><span class="n">network</span>
    <span class="n">depends_on</span><span class="p">:</span>
      <span class="o">-</span> <span class="n">spark</span><span class="o">-</span><span class="n">master</span>
      <span class="o">-</span> <span class="n">spark</span><span class="o">-</span><span class="n">mongo</span>
<span class="n">networks</span><span class="p">:</span>
  <span class="n">spark</span><span class="o">-</span><span class="n">network</span><span class="p">:</span>
    <span class="k">external</span><span class="p">:</span>
      <span class="n">name</span><span class="p">:</span> <span class="n">spark</span><span class="o">-</span><span class="n">network</span>
<span class="n">volumes</span><span class="p">:</span>
  <span class="n">mongodb</span><span class="p">:</span>
  <span class="n">mongodb_config</span><span class="p">:</span>
</code></pre></div>


<p>Now, if you have set-up the entire thing correctly, you can start a spark cluster with the following command <code>docker-compose up</code>. We will be using <em>mongo</em> as a way to store our data, but you could use any other db. To check if everything went correctly, you can enter the next address on your web-browser: <code>http://localhost:8080/</code> you should see a running spark-master alongside two registered workers :D. In the next blog post we will see how to submit your spark job to your recently created cluster using a <code>makefile</code> that will make our lives easier next time we would want to execute our jobs.</p></div>
    
    <br>
    <!-- Needed for disqus-->
    <div id="disqus_thread"></div>

  </div>

        
        <!-- Footer -->
        <div class="border-top pt-3">
          <small class="text-muted">
              Made with ❤️ using flask, frozen-flask, Bootstrap
          </small>

          <div class="right-footer">
            <a href="https://github.com/charx7" style="color: black!important; text-decoration: none;">
              <i class="fa fa-github" style="padding-right: 16px"></i>
            </a>
    
            <a href="https://www.linkedin.com/in/carlos-rodolfo-huerta-santiago-2b5609100/" style="color: black!important; text-decoration: none">
              <i class="fa fa-linkedin" style="padding-right: 16px"></i>
            </a>
          </div>
          
        </div>
      </div>
    </div>
    
    <!-- jQuery -->
    <script
			  src="https://code.jquery.com/jquery-3.5.1.min.js"
			  integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0="
        crossorigin="anonymous"></script>
    <!--  BS js -->
    <script type="text/javascript"
      src="/static/js/bootstrap.bundle.js"></script>

    
<!--
<script>
  (function() { // DON'T EDIT BELOW THIS LINE
  var d = document, s = d.createElement('script');
  s.src = 'https://carlos-huerta-blog.disqus.com/embed.js';
  s.setAttribute('data-timestamp', +new Date());
  (d.head || d.body).appendChild(s);
  })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
-->                          


  </body>
</html>